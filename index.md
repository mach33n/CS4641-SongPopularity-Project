# CS 4641 Project
Spotify is a media streaming platform that allows users to listen to upwards of 60 million songs. Spotify provides users with information about songs like music genre, song length, artist, album. By tallying the number of individual streams of a song (or any piece of media), Spotify also provides users (and artists) with a metric of how popular a song is. Behind the scenes, Spotify keeps track of features of songs including modality, “danceability, energy, and tone. We have reason to believe that these features created by spotify are capable of predicting some level of popularity amongst other song titles with similar features. We have examples of analysing these sorts of features to support us [1,4]. 

# Problem Statement
Spotify has been an ever improving platform for a while now and currently holds the top spot among audio streaming platforms. Many artists recognize the importance of such a platform and have been utilizing the platform even more to attract listeners and build fanbases. 
Given that Spotify measures a large variety of information about the songs on its platform, we intend to investigate how the relationships between these features, both front-facing and not, interweave to affect the popularity of music upon release by predicting the popularity of a song given certain features. These predictive capabilities would be useful for artists and labels looking to optimize their music outputs in an effective way [1]. Though some other sources exist that have intended to achieve a similar goal, we hope to use a combination of unsupervised and supervised learning to achieve a more optimal and accurate analysis.

# Data Collection
A single Kaggle dataset was used throughout the course of this project midpoint [5]. All steps taken to ensure the data set be properly cleaned and prepared for any machine learning algorithms were taken with respect to how they would assist in solving our problem. The first step taken was to read in our data set as a csv and ensure all data types were properly assigned to each feature. Seeing as the label of choice(popularity), was pre-recorded in a range of 0-100, it was left as an integer though considerations were made as to turning it into a floating point probability. This was decided against as chosen algorithms were likely to be supervised regressions and having that range would be more presentable in some graphs. After assigning types, string metrics such as artist name were modified to remove extraneous characters(i.e., brackets, commas, etc). Numeric features with odd units were changed to simpler units(e.g., duration_ms was converted to duration_sec), and release dates were cleaned to just include the year. Once complete, feature importance received its first considerations with respect to the problem definition. Most data columns provided numeric data of hardly identifiable metrics provided by spotify’s API so those were left in the data as there was no reason to consider them useless with the exception of a release-date column which was deemed not very useful in its form therefore it was dropped. Columns of text such as id and song name were removed as they provided no real significance in predicting song popularity. The artist name column was removed as well however it was noted that this feature should be kept around as it could be used to generate more features via third-party APIs such as Chartmetric. With these columns removed, re-analysis of numeric columns took place and it was decided that data instances containing song durations of over 20 minutes(1200sec) should be removed as they will skew the data unfavorably(based on some histogram analysis and group discussion). In addition, the categorical feature representing the key of each song was converted to be one-hot encoded as the song keys were independent in their effect on individual songs and how they affect listening experience. After making these higher level decisions, lower level analysis in the form of histograms, correlation matrices and scatter plots occurred. It was noticed that a numerical feature “instrumentalness” had a large skew towards 0 represented in the data set which gave cause to believe this value was not recorded effectively in the initial acquisition and for that reason should be dropped. 
<img src="images/instrumentalness.png" alt="pearson correlation heatmap" class="center" style="width: 360px; height: 360px; margin-left: auto; margin-right: auto; display: block;"/>

Analysis of all other numerical features displayed reasonable distributions with the exception of our chosen label, popularity. 
<img src="images/sparse_mat.png" alt="pearson correlation heatmap" class="center" style="width: 360px; height: 360px; margin-left: auto; margin-right: auto; display: block;"/>
There was a similarly large presence of songs with 0 popularity which could be the result of skewed song selection by year or some other data input error but understandably this data is necessary for solving the defined problem therefore it was decided that any further analysis should be halted while this data was split into even sections as it would be in a training/testing data split. The thought process was that, if this data was sectioned evenly with regard to popularity and then analysed for correlations then model results would be more accurate when testing and hopefully equally as accurate when provided new data. In order to split this data evenly with respect to our label, a categorical feature was created using a quantile-based discretization function which created 4 bins of data with respect to popularity. After generating these bins, a stratified shuffle sampling was used to ensure even splits of data occurred with respect to each bin based on a test size ratio of 0.2. To verify this data was evenly split, we printed out the percentage of presence from each popularity bin in our training data and the total data which displayed results that were equal to the 3rd decimal place! 

<img src="images/even_split.JPG" alt="pearson correlation heatmap" class="center" style="width: 360px; height: 240px; margin-left: auto; margin-right: auto; display: block;"/>

After splitting this data, the test set was put aside and further analysis occurred on the training set. Through the use of pearson’s correlation matrix it was discovered that some features which did not have a strong correlation with popularity had a strong correlation with other features.

<img src="images/heatmap.png" alt="pearson correlation heatmap" class="center" style="width: 480px; height: 360px; margin-left: auto; margin-right: auto; display: block;"/>

4 of these instances were analyzed and through the use of some experimental testing, 3 of these instances produced extra features with above 24% correlation to popularity which was very exciting to see. Most of the initially provided features lacked much description and any unit input therefore they are not easily justifiable as to the relationships discovered in them though they were added with the hope hidden trends would be revealed. This would conclude the data exploration and cleaning phase.

# Methods
The methods of data collection were quite simple. After downloading the dataset from Kaggle, we used Pandas to clean our data. Non-numeric values such as song ID and song name were removed by dropping the column, and the names of the artist were trimmed to remove the extraneous symbols that were included in the dataset using string splicing. The transition of the length of the song from milliseconds to seconds were achieved through mathematical operations on the entire column, and we then followed up by disregarding all songs that were longer than 20 minutes by dropping the row. The creation of one-hot encoded variables in order to account for the song’s key in its popularity was done by creating new rows that were set to either 0 or 1 based on the song’s key. After cleaning our data, we then displayed the Pearson Correlation Table for our data using Pandas’ corr method. 

After cleaning our data, we applied PCA (using the scikit implementation) to it to reduce its dimensionality. We made use of MLE (through the scikit function parameter) to determine an appropriate amount of dimensions from our original data. 

We made use of three different types of supervised learning: a decision tree, a linear regression model, and a neural network. The decision tree was made using the sklearn tree class DecisionTreeRegressor object, with the goal being to learn decision rules from data features, assuming there is a non-linear/complex relationship. The linear regression model was created using the sklearn linear_model LinearRegression class, with the goal being to map a linear relationship between observed targets and predicted points. The neural net was made using Keras with an Adam (adaptive moment estimation) optimizer rather than an SGD procedure, a ReLU activation function on the first three layers, and a linear activation function on the last layer. For all of these model types, we ran the data on the PCA data as well as on the original data as a gauge of PCA’s efficacy in hopes of yielding better results.
<img src="images/model_summary.png" alt="nn model summary" class="center" style="width: 480px; height: 360px; margin-left: auto; margin-right: auto; display: block;"/>

# Potential Results
With our model, we hope to measure the chance of a song becoming popular based on attributes of the song such as its danceability, energy, and tempo found in the database we'll be using. By first determining which attributes are the most important when determining the popularity of a song, the model can then determine if a inputted song has the perfect combination of features, predicting whether a song will likely be remembered as a bop or gather dust in the cloud.

# Discussion
Our chosen datasets are not bound to cover the same range of songs so it is highly likely that we will have to acquire additional data from the respective APIs to have a consistent set of songs that we can evaluate metrics on. Sanitizing the datasets will be important given the ratio of popular to unpopular songs is not equal. Expanding on the issue of popularity, the datasets will likely contain songs that do not equally have a balance of metrics (we expect more songs from the “pop” genre etc.) and in the process of scraping more songs, we need to balance our dataset. 


# References
[1] http://cs229.stanford.edu/proj2015/140_report.pdf 

[2] https://github.com/MattD82/Predicting-Spotify-Song-Popularity

[3] https://dl.acm.org/doi/pdf/10.1145/3068335 

[4] https://arxiv.org/pdf/1908.08609.pdf 

[5] https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks



